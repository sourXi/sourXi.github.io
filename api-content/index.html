{"posts":[{"title":"数据可视化之作业总结","content":"很喜欢数据可视化这门课，感觉是集艺术与科技之大成。这篇来简单梳理下自己的作业。（暂时是贴图形式，未来希望可以部署到服务器上，时时刻刻都能打开555） basic直方图 第一个实际接触svg操作的作业，数据简单，图形简单，发现圆角矩形，时间花在调色上了，无语。 basic 随机树 递归实现随机树。服气，反正前几次作业基本时间上都花在调色上了，是想当什么调色大师吗，明明有写好的api你不查。哦对，递归层数不能太多，可能是CPU拉跨的原因吧，树长得就不能很茂密……CPU和GPU原来是土壤和水源啊。 basic 随机文字树 与上图使用技术基本相同，就是把自底向上改成了自顶向下……数据部分主要在传承家族荣光哈哈哈orz D3直方图 作为数据可视化的一个d3作业，使用的数据比较简单。由于不知道有colorScheme这种东西，于是花了好久调颜色orz D3饼图与直方图+链接数据库 这个作业其实分了两次完成。第一次是仅完成饼图和直方图的绘制，第二次加入了链接数据库的操作，which 在真实场景中更常见。第二次的svg图形也在第一次基础上有改动，这里只放出第二版。在本作品中，有3个交互场景： - 鼠标悬停在不同bar上，会显示对应数据 - 鼠标悬停在不同pie上，会显示对应百分比 - 鼠标点击不同pie，bar chart会切换为对应国家的图形 （其实或许可以加一个：点击不同图例，bar切换为对应国家的图形 ","link":"https://sourxi.github.io/post/shu-ju-ke-shi-hua-zhi-zuo-ye-zong-jie/"},{"title":"OpenCV+dlib初探人脸识别 ","content":"引言 刚刚写了6分钟，结果不小心点了叉叉没保存😅 简单再写下，总之就是感觉调库调包就是深度学习的最终归属。不要总是带着动辄大几百MB的weights和infer/test代码，调试起来就是很烦啊！这样几句话就解决不香吗。当然效果有待提升哈。 demo1——OpenCV 需要在代码加入包D:/ComputerScience/Python/Python36/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml 安装OpenCV pip install opencv-python 代码 import cv2 def detect(filename): face_cascade = cv2.CascadeClassifier('D:/ComputerScience/Python/Python36/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml') img = cv2.imread(filename) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray, 1.3, 5) for (x, y, w, h) in faces: img = cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2) cv2.imshow('Person Detected!', img) cv2.waitKey(0) cv2.destroyAllWindows() if __name__ == '__main__': detect('hp1.jpg') 结果 which should be like this…… 可以看到jl大姐被可怜的忽视掉了，就是因为这个结构化或者说规范化的算法不包含旋转一定角度之后的正脸特征。 如果我们把刚才改了的包改成识别眼镜，which is haarcascade_eye.xml，那么结果如下： which就挺无语的哈哈哈 demo2——dlib+face_recognition 安装dlib及依赖 pip install openblas cmake dlib face_recognition 代码 #coding=utf-8 import cv2 import dlib path = &quot;hp1.jpg&quot; img = cv2.imread(path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #人脸分类器 detector = dlib.get_frontal_face_detector() # 获取人脸检测器 predictor = dlib.shape_predictor(r&quot;D:/ComputerScience/Python/Python36/Lib\\site-packages\\shape_predictor_68_face_landmarks.dat&quot;) dets = detector(gray, 1) for face in dets: shape = predictor(img, face) # 寻找人脸的68个标定点 # 遍历所有点，打印出其坐标，并圈出来 for pt in shape.parts(): pt_pos = (pt.x, pt.y) cv2.circle(img, pt_pos, 2, (0, 255, 0), 1) cv2.imshow(&quot;image&quot;, img) cv2.waitKey(0) cv2.destroyAllWindows() 结果 还挺惊悚的哈哈哈。不过暂时除了特效一些场景还没想到能怎么比较好的去应用它。 结束啦 感觉代码什么的其实还没好好看，不过综合最近尝试的VOS模型来讲，这种也太好用了吧！！DL要加油喔！！ ","link":"https://sourxi.github.io/post/opencvdlib-chu-tan-ren-lian-shi-bie/"},{"title":"Node.JS+Vue+Vite+D3 笔记","content":"认识一下🙌 Node.js 一个基于Chrome V8引擎的JavaScript运行环境，使用了一个事件驱动、非阻塞式I/O模型， 让JavaScript 运行在服务端的开发平台。 Vue The Progressive JavaScript Framework目前最为流⾏的前端框架之⼀，不可否认的是它相比React 学习曲线要更为平缓。 Vite Native-ESM powered web dev build tool. It's fast. 面向未来的前端构建工具。（Web开发构建工具、快速的冷启动、即时热模块更新、真正按需编译） D3.js 是一个可以基于数据来操作文档的 JavaScript 库。（数据可视化用） Npm 包管理工具（官方） Yarn 包管理工具（第三方） Node.js 初尝试👊 [下载地址] 控制台输入node --version验证是否已安装成功 创建js文件 hello.js var http=require(&quot;http&quot;); http.createServer(function(request,response){ response.writeHead(200,{'Content-Type':'text/plain'}); response.end('Hello World\\n'); }).listen(8888); console.log('Sever running at http://127.0.01:8888/'); 在hello.js所在目录下，运行文件 浏览器打开http://127.0.01:8888 安装yarn✂️ 执行命令npm install yarn git也是必须的，win10这里安装过，安装包去官网就可以找到 Vue/Vite 初尝试👊 启动脚手架 yarn create @vitejs/app vue-d3-test 生成预置的快速启动模板. 进入项目目录cd a-test-app，yarn build + yarn dev使用开发模式启动 网址输入http://localhost:8080 Vue目录结构 node_modules 不用管，极其不重要。即便手贱删掉，再次启动的时候也会🗡🗡地贴回来😃 public 静态资源目录。大概就是图片啊、json啊什么的放在这里就好了 src 重头戏来了。 assets 放置图片，如logo； components 放置组件文件。会在App.vue中以标签方式引用； App.vue 项目入口文件，由各种组件拼接在一起组成； index.css 页面的样式文件； main.js 主要负责最终html页面中的body部分； index.html 网页文件，js部分引用main.js。 D3直方图📊 安装D3 yarn add d3 安装axios yarn add axios 代码如下 BarChart.vue &lt;template&gt; &lt;h2&gt;201811153001 孙圆希 直方图&lt;/h2&gt; &lt;div id=&quot;bar-chart-container&quot;&gt;&lt;/div&gt; &lt;/template&gt; &lt;script&gt; import { defineComponent } from &quot;vue&quot;; import axios from &quot;axios&quot;; import * as d3 from &quot;d3&quot;; export default defineComponent({ data() { return { color: &quot;steelblue&quot;, margin: { top: 30, right: 0, bottom: 30, left: 40 }, }; }, /** * 在挂载后即开始执行 */ mounted() { axios.get(&quot;./alphabet.json&quot;).then((res) =&gt; { const barChartData = Object.assign(this.formatData(res.data), { format: &quot;%&quot;, y: &quot;↑ Frequency&quot;, }); this.drawBarChart(barChartData); }); }, methods: { /** * 格式化数据 */ formatData(data) { return data .map(({ letter, frequency }) =&gt; { return { name: letter, value: frequency }; }) .sort((a, b) =&gt; d3.descending(a.value, b.value)); }, /** * 绘制直方图 */ drawBarChart(data) { const margin = this.margin; const width = 800; const height = 500; // 初始化 SVG 元素 const svg = d3 .select(&quot;#bar-chart-container&quot;) .append(&quot;svg&quot;) .attr(&quot;class&quot;, &quot;bar-chart&quot;) .attr(&quot;viewBox&quot;, `0 0 ${width} ${height}`) .attr(&quot;width&quot;, width) .attr(&quot;height&quot;, height) .append(&quot;g&quot;); // https://observablehq.com/@d3/d3-scaleband // x 轴的缩放比例尺 const x = d3 .scaleBand() .domain(d3.range(data.length)) .range([margin.left, width - margin.right]) .padding(0.1); // y 轴的缩放比例尺 const y = d3 .scaleLinear() .domain([0, d3.max(data, (d) =&gt; d.value)]) .nice() .range([height - margin.bottom, margin.top]); // x 坐标轴 // tickSizeOuter(0) 移除 0 处初始的标记 // tickFormat https://github.com/d3/d3-scale/blob/master/README.md#tickFormat const xAxis = (g) =&gt; g.attr(&quot;transform&quot;, `translate(0,${height - margin.bottom})`).call( d3 .axisBottom(x) .tickFormat((i) =&gt; data[i].name) .tickSizeOuter(0) ); // y 坐标轴 const yAxis = (g) =&gt; g .attr(&quot;transform&quot;, `translate(${margin.left},0)`) .call(d3.axisLeft(y).ticks(null, data.format)) // 移除区域间的竖线 .call((g) =&gt; g.select(&quot;.domain&quot;).remove()) .call((g) =&gt; g .append(&quot;text&quot;) .attr(&quot;x&quot;, -margin.left) .attr(&quot;y&quot;, 10) .attr(&quot;fill&quot;, &quot;currentColor&quot;) .attr(&quot;text-anchor&quot;, &quot;start&quot;) .text(data.y) ); svg .append(&quot;g&quot;) .attr(&quot;fill&quot;, this.color) .selectAll(&quot;rect&quot;) .data(data) .join(&quot;rect&quot;) .attr(&quot;x&quot;, (d, i) =&gt; x(i)) .attr(&quot;y&quot;, (d) =&gt; y(d.value)) .attr(&quot;height&quot;, (d) =&gt; y(0) - y(d.value)) .attr(&quot;width&quot;, x.bandwidth()); // 绘制到 SVG svg.append(&quot;g&quot;).call(xAxis); svg.append(&quot;g&quot;).call(yAxis); }, }, }); &lt;/script&gt; App.vue &lt;template&gt; &lt;img alt=&quot;Vue logo&quot; src=&quot;./assets/logo.png&quot; /&gt; &lt;HelloWorld msg=&quot;Hello&quot; /&gt; &lt;bar-chart /&gt; &lt;test-axios /&gt; &lt;/template&gt; &lt;script setup&gt; import HelloWorld from &quot;./components/HelloWorld.vue&quot;; import TestAxios from &quot;./components/TestAxios.vue&quot;; import BarChart from &quot;./components/BarChart.vue&quot;; &lt;/script&gt; 结束 至此，我们的vue+d3可视化项目页面大概长这样。 我在学习中的感受是，vue热重载真的很好用，u1s1比我现在写博客的这个还好用，不用考虑浏览器数据什么的真的很香。不过vue框架要想用好用熟绝不是这一两节课就能一蹴而就的，有时间的话好好研究一下应该会挺有趣的啦👽👻 ","link":"https://sourxi.github.io/post/nodejsvuevited3-bi-ji/"},{"title":"STEm-Seg论文阅读","content":"可能总是要给自己找点事做，于是最近误打误撞地在打天池的视频云比赛。 一直以来我对CV方向的理解都仅仅来源于知乎上的“黑帖”……以为CV已经到头了、不能更卷之类的。虽然对图像和图形学十分感兴趣，但一直敬而远之，没能深入。（我真的好想抽出一大段时间做动画啊哭o(╥﹏╥)o） 这次偶然接触了cv，感觉其实很好玩！未来也有不错的应用场景（饼）。至于比赛，这次是关于视频目标检测与分割的部分，之前已经阅读了很多Video Object/Instance Segmentation相关的paper，很多都不怎么合适。初赛阶段也一直只是跑跑卡丁车代码，感觉复赛再没有算法贡献就要被组委会踢了……所以下面总结一下最近效果还不错的一篇paper：STEm-Seg。[paper][code] 动机 之前的网络大多top-down + tracking-by-detection模型有多个网络组成，算力需求大，不是端到端。 创新 bottom up + spatio-temporal 类别agnostic，同一物体的像素聚类 适应多场景欠标注条件 聚类 单网络 方法 输入： T*W*H（N）*3 输出： ①每个像素的embedding N*E ②每个像素和每个维度的方差N*E 3⃣️ object instance center heatmap Instance表示： 1. 计算均值+方差 2. 计算像素属于该类别的概率 3. threshhold p&gt;0.5 训练： 可训练的参数——均值&amp;方差；目标函数——IoU（4）；loss函数——三个 Embedding表示： 自身+位置信息（xi，yi，ti），2d变3d，也可以free dimension，但不是越多越好 Infer 流程： （1）根据Heatmap选出instance的中心点； （2）找到该中心点所对应的embedding和方差向量； （3）将该instance的均值和方差用这个中心点所对应的来表示，并通过计算公式（2），来判断某个pixel属于该实例的概率； （4）由于一些像素已经分配到实例j中，所以为了更长远的考虑，将他们的信息剔除掉 （5）重复1-4的操作，直到Heatmap为空集，或者最大值小于我们设定的threshold。 内存限制，视频切分 网络结构： 1 encoder + 2 decoders encoder：FPN 4 scales decoder：ResNet101 3d conv + pooling（内存：size大的layer少） 一些问题 heatmap是怎么学习到物体center的？open H(ci)代表probability，argmax H(ci)代表概率最高的点，也就是center吧。 追问：H是啥？看了代码，认为H只是在做几次down sample，结果可以看做一个feature map或者就是conv+pooling，但是很多地方都称为heatmap。但是与center的关系？ 训练过程里embedding head是在优化mean和covariance，如何理解这个covariance？ 边界？closed 方差小的像素点和均值点比较相似，更容易属于同一物体；方差大的地方差异大，变化剧烈，可以被划成其他物体。 infer过程中的step 2：“Find the corresponding embedding vector E(cj) and variances V(cj).”不存在点的集合，如何计算方差V(cj)？closed 集合是图片中所有像素点，均值是计算所有点的均值。得到中心点后，以中心点为均值，计算对应方差，从而得到预测的mask。最后把mask中的点从图片中抠掉。重复。 上述是猜测，未代码证实。如果是这样，那么有一点疑惑：semantic segmentation？还是要搞清楚Heatmap到底咋算的。只有animals？图让我又傻了。 续：有threshhold啊笨蛋。背景什么的值会非常低，但是那时已经不会再考虑了。以及后续实验验证，threshold调低效果会更好…… 调到0.1之后分数直接涨了1% orz…… ","link":"https://sourxi.github.io/post/stem-seg-lun-wen-yue-du/"},{"title":"Self-driving Udacity","content":"https://www.youtube.com/watch?v=EaY5QiZwSP4 https://github.com/llSourcell/How_to_simulate_a_self_driving_car ","link":"https://sourxi.github.io/post/self-driving-udacity/"},{"title":"关于","content":" 欢迎来到我的小站呀，很高兴遇见你！🤝 🏠 关于本站 坚持巴拉巴拉写，拒绝糊里糊涂学🤡 👨‍💻 博主是谁 数据科学在读本科生👶 ⛹ 兴趣爱好 很多，但是和专业相关的没几个😐 📬 联系我呀 隔空喊话叭💻 ","link":"https://sourxi.github.io/post/about/"},{"title":"DAML： Dual Attention Mutual Learning between Ratings and Reviews for Item Recommendation","content":"📎📎📎 Liu, Donghua and Li, Jing and Du, Bo and Chang, Jun and Gao, Rong. 2019. DAML: Dual Attention Mutual Learning between Ratings and Reviews for Item Recommendation. Association for Computing Machinery. 344–352. 📎📎📎 DAML: 项目推荐评分与评价的双注意互学习 👀 摘要 ​尽管有很多基于矩阵分解的协同过滤方法都取得了巨大成功，但是推荐系统领域仍然有着很大的进步空间。其中一个主要的困难就是冷启动和数据稀疏问题需要更好的解决方案。最近的研究中开始尝试结合评论来做评分预测。然而，还有两大问题需要解决：（1）大多数现有的研究都采用了一种静态和独立的方法来提取用户和评论的隐性特征表示，忽略了这些隐形特征之间的相互关联，从而导致无法全面捕捉到用户的偏好。（2）现在还没有统一评论和评分的有效框架。因此，我们提出了一个新的项目推荐评分与评价的双注意互学习。特别地，我们使用卷积神经网络的局部和相互注意力来提高DAML模型的可解释性。这样评分特征和评论特征就被整合成一个统一的神经网络模型，并且特征之间的高阶非线性相互作用可以通过神经因子分解机来实现，从而完成最终的评分预测。通过在五个真实数据集上的实验表明DAML在评分预测准确率上比其他最新方法都更高。此外，注意力机制可以高亮出评论中的相关信息来提高评分预测的可解释性。 1 引言 用户评分作为一种反映用户兴趣的用户反馈，已经被广泛用于预判用户的偏好。矩阵因子化(MF)在学习用户偏好方面取得了巨大的成功[9，24]。依靠用户与物品的交互记录，MF方法将用户的偏好和物品的特征表示为隐式空间中的隐式因子向量，进行评分预测。然而，评分只反映了用户对某一项目的整体满意度，而没有解释用户为何对该项目进行这样的评分，导致无法解释的推荐和冷启动问题[4，22]。为了解决这些局限性，研究者们对评论进行了关注。与评分相伴的评论通常包含与用户偏好和物品属性相关的各种信息。最近，深度学习模型被提出，来从评论中提取有效的隐式特征来进行评分预测[18，30]。在这些作品中，采用卷积神经网络(convolutional neural networks, CNNs)来捕捉评论更有效的上下文特征。虽然这些研究比基于话题的模型取得了更好的推荐性能，但也有一些问题没有被彻底研究。 现有的大多数作品都是以静态和独立的方式学习用户和项目的隐式特征[2，18，30]。然而，用户和项目的评论文本通常包含了用户和项目相关的语义信息，而没有考虑它们之间的特征相关性，这可能导致预测用户的偏好有很大偏差[26]。这可能导致预测用户偏好的偏差很大[26]。 融合方法有两种。一种是传统的基于MF或因子分解机(FM)的数据融合方法[16]。然而，这种方法无法捕捉到不同模式中特征之间的复杂性[7，21]。另一种是直接将从不同数据源中提取的特征同等对待，将它们有序连成一个特征向量，用于推荐任务。然而，单纯的向量连接并没有考虑到潜伏特征之间的相互作用，这对于建模协作过滤效果是不够的[31]。 为了解决上述问题，我们提出了一种DAML模型。 该模型利用CNN的注意力机制和多层感知器(MLP)的非线性来实现对用户的预测评级。这篇文章的贡献总结如下： 所提出的DAML模型采用双注意层：局部注意层和相互注意层。前者在卷积层之前使用，从本地窗口中选择有助于项目属性和用户偏好的词。后者在卷积层之后被利用，学习用户评论文本和商品评论文本之间的相关语义信息，实现用户和商品的动态交互。 我们提出了一个统一的神经网络模型来融合评分和评论。我们提出了统一的神经网络模型来融合评分和评论，然后利用神经因子化机器来实现潜伏特征的高阶非线性交互。通过端到端的多任务学习方法对模型参数进行训练。 实验在5个真实世界的数据集上进行，实验结果表明，提出的DAML模型比现有的最先进的方法达到了更好的评分预测精度。进一步的研究表明，DAML在评论中强调的词是非常有意义的，可以揭示用户对某一物品的具体偏好，这有助于提高推荐系统的可解释性。本文的其余部分组织如下。第2节回顾了推荐系统的相关工作。第3节介绍了问题的提出；第4节详细介绍了DAML模型的整体框架，与之配套的是学习算法。第5节介绍了实验设置和结果。第6节总结全文。 2 相关工作 我们的工作与两条文献线有关，即用于推荐的评论文本和神经网络。我们回顾了这两个领域的最新进展。 2.1 推荐中的评论文本 传统的协同过滤推荐算法有两个显著的缺点：一是数据稀疏性，二是冷启动问题。随着用户与系统平台之间交互性的增加，一些与用户和物品密切相关的辅助信息被利用来缓解上述弊端，尤其是评论文本已经成为提高推荐系统性能的研究热点。一些作品在评论上利用话题模型技术，如Latent Dirichlet Allocation(LDA)[1]，并将潜在的话题和评分进行耦合[3，4]，这比单纯利用评分或评论的基线有明显的改善。但这些研究忽略了评论的词序和局部语境信息，大量的短语和句子形式的具体信息被丢失[9，30]。同时，这些研究采用线性方法而不是非线性方法[8，11，19]来整合评论和评分进行评分预测，不足以捕捉非线性和复杂的特征交互结构。 2.2 推荐中的深度学习 深度学习技术在推荐系统中取得了巨大的成功。He等人利用多层MLP，通过最大化用户与物品的交互作用来提取高级隐藏特征，实现用户与物品特征的非线性交互[7，8]。Wang等[14，25]利用神经网络学习评论的深层特征表示，并利用概率矩阵因子化（PMF）[17]完成评分预测。但是，该工作仍然采用词袋[5]表示法来学习潜在的主题。为了提高深度特征表示的能力，采用词向量模型和CNN来学习用户行为和项目属性表示[2，9，30]。但是，它们仍然是以静态和独立的方式学习用户和项目的潜伏特征，无法学习潜伏特征的相关性。受关系预测和深度学习[12，18，26]的启发，我们提出了一种用于物品推荐的DAML模型。 所提出的DAML模型与上述模型在几个方面有很大不同。首先，利用双重注意机制来学习用户与项目之间的交互。局部注意层关注句子中不同词的重要性，而相互注意层则关注特征交互的学习。其次，我们没有应用注意力矩阵来推导特征的关系，而是提出了一种新的方法，即定义一个相关评分函数来计算特征的相关性，可以更直观地捕捉到用户和物品之间的相关性。然后我们利用神经网络将评分和评论统一起来，通过一些给定的规则进行评分预测。我们的实验结果表明，DAML的性能优于最先进的方法。 3 问题形式 图1是DAML模型的架构。让 uuu 和 U={u1,u2,...,ui,...,uM}U = \\{u_1,u_2, ... ,u_i , ... ,u_M \\}U={u1​,u2​,...,ui​,...,uM​} 分别表示一个用户和整个用户集；同理，iii 和 I={i1,i2,...,ij,...,iN}I = \\{i_1,i_2, ... ,i_j , ... ,i_N \\}I={i1​,i2​,...,ij​,...,iN​} 分别用来表示一个物品和整个物品集。用户对物品的评分 R∈RM×NR∈\\mathbb{R}^{M×N}R∈RM×N 表示他们之间的交互，可以是实值化的显性评分，也可以是二进制的0/1隐性反馈。在这里，我们研究的是显式评级，其中每个 ru,i∈Rr_{u,i}∈Rru,i​∈R 都是一个实值。让xxx和 X={x1，x2，...，xi，...，xM}X=\\{x_1，x_2，... ，x_i ，... ，x_M \\}X={x1​，x2​，...，xi​，...，xM​} 分别表示一个用户评论文本和整个用户评论文本集；同样，让sss和 S={s1，s2，...，sj，...，sN}S=\\{s_1，s_2，... ，s_j ，... ，s_N \\}S={s1​，s2​，...，sj​，...，sN​} 分别表示一个物品评论文本和整个物品评论文本集。DAML模型可以形式化如下。 输入： 交互数据的输入是用户和物品的ID。我们使用one-hot编码的稀疏向量 νuUν^U_uνuU​ 和 νiIν^I_iνiI​ ，分别描述用户 u∈Uu∈Uu∈U 和物品 i∈Ii∈Ii∈I 。评论数据的输入分别是 xu∈Xx_u∈Xxu​∈X 为用户 uuu 的评论数据集，si∈Ss_i∈Ssi​∈S为物品 iii 的评论数据集。 输出： 整个训练过程可以用函数表示：fu:U,X,I,S→R^f_u : U,X, I, S → \\hat Rfu​:U,X,I,S→R^ 。模型的输出是最终的预测评分R^\\hat RR^。即对于任何用户uuu，我们可以根据函数fu:νuU，νiI，xu，si→r^u,if_u : ν^U_u ，ν^I_i ，x_u，s_i → \\hat r_{u,i}fu​:νuU​，νiI​，xu​，si​→r^u,i​，得到预测评级r^u,i\\hat r_{u,i}r^u,i​。 4 提出模型 模型中有两个部分：一个是特征学习模块；另一个是特征交互模块。 特征学习模块。 特征学习模块包括两部分： 基于评论 的特征学习部分和 基于交互 的特征学习部分。基于评论的特征学习部分利用CNN的注意力机制来学习用户偏好和物品特征之间的相关性。基于交互的特征学习部分用于将用户和物品映射成低维密（dense）向量，以捕捉特征的非线性相互作用。 特征交互模块。 在特征交互模块中，我们将评分和评论特征整合成统一的神经网络模型。然后利用神经因子机对潜伏特征向量之间的高阶非线性交互进行建模。 4.1 特征学习模块 4.1.1 基于评论的特征学习 基于评论的特征学习利用CNN的卷积操作和注意力机制，学习用户和项目特征之间的相关语义信息。图2是基于评论的特征学习的架构。 Embedding lookup layer. 给定评论文本 xu∈Xx_u∈Xxu​∈X ，它由 lll 个词组成，可以表达完整的意思。在嵌入层中，利用词向量模型Glove[15]将 xux_uxu​ 中的每一个词映射成词向量 wiw_iwi​ ，然后将这些词按照在评论文本中出现的顺序进行串联，形成评论文本矩阵 D∈Rd×lD∈\\mathbb R ^{d×l}D∈Rd×l ，并保留词的顺序。 (1)D=[...,wi−1,wi,wi+1,...]D=[...,w_{i-1},w_i,w_{i+1},...] \\tag{1} D=[...,wi−1​,wi​,wi+1​,...](1) 其中 ddd 为每个词的嵌入维度， wiw_iwi​ 代表 xux_uxu​ 中第 iii 个词的词向量。 Local attention layer. 受工作[18]的启发，我们利用注意力滑动窗口来学习评论中每个词的权重。我们将词向量矩阵 D∈Rd×lD∈\\mathbb R ^{d×l}D∈Rd×l 中的第 iii 个词作为中心词，ωωω 为滑动窗口的宽度。第 iii 个词的关注权重 s(i)s(i)s(i) 可以通过参数矩阵 WL_aW_{L\\_a}WL_a​ 和偏置 bL_ab_{L\\_a}bL_a​ 计算，如下所示。 wL_a,i=(wi+−ω+12,wi+−ω+32,...,wi,...,wi+ω−32,wi+ω−12)w_{L\\_a,i}=(w_{i+{-\\omega+1 \\over 2}},w_{i+{-\\omega+3 \\over 2}},...,w_i,...,w_{i+{\\omega-3 \\over 2}},w_{i+{\\omega-1 \\over 2}}) wL_a,i​=(wi+2−ω+1​​,wi+2−ω+3​​,...,wi​,...,wi+2ω−3​​,wi+2ω−1​​) (2)s(i)=δ(wL_a,i)WL_a+bL_as(i)=\\delta (w_{L\\_a,i})W_{L\\_a}+b_{L\\_a} \\tag{2} s(i)=δ(wL_a,i​)WL_a​+bL_a​(2) 其中 δδδ 表示非线性激活函数，我们用sigmoid函数作为激活函数. s(i)s(i)s(i) 是第 iii 个词嵌入的注意力权重，可以用来判断该词在句子中的重要性。根据注意力权重，第 iii 个词的词向量 w^i\\hat w_ iw^i​ 可以计算如下: (3)w^i=s(i)wi\\hat w_ i=s(i)w_i \\tag{3} w^i​=s(i)wi​(3) 其中 s(i)s(i)s(i) 和 wiw_iwi​ 分别为第 iii 个词的注意力权重和词向量。局部注意力权重的词向量矩阵可以表示为： (4)D^=[...,w^i−1,w^i,w^i+1,...]\\hat D=[...,\\hat w_{i-1},\\hat w_{i},\\hat w_{i+1},...] \\tag{4} D^=[...,w^i−1​,w^i​,w^i+1​,...](4) Convolution operation. 给定词向量矩阵 D^\\hat DD^，卷积操作用于提取语义信息。具体来说，利用第 jjj 个卷积滤波器的滑动窗口大小 ωωω 来提取局部上下文特征 cijc^j_icij​ 。 (5)cij=Wcj∗D^(:,i:(i+ω−1))c^j_i=W_c^j*\\hat D_{(:,i:(i+\\omega-1))} \\tag {5} cij​=Wcj​∗D^(:,i:(i+ω−1))​(5) 其中，∗∗∗为卷积运算；WcjW_c^jWcj​表示第 jjj 个卷积滤波器的卷积权重向量，D^(:,i:(i+ω−1))\\hat D_{(:,i:(i+\\omega-1))}D^(:,i:(i+ω−1))​ 为从第 iii 个位置开始的滑动窗口内矩阵 D^\\hat DD^ 的片断。为了产生 lll 个上下文特征，我们首先在卷积操作前在矩阵 D^\\hat DD^ 的末尾垫上 ω−1ω - 1ω−1 个零向量。由于卷积窗口中的共享权重只能捕获一种类型的上下文特征，我们使用多个卷积滤波器，用不同的卷积权重来捕获每个词的上下文特征。在卷积运算后，第 iii 个位置的上下文特征可以表示为 cic_ici​ : (6)ci=[ci1,ci2,...,cii,...,cif]\\pmb {c_i}=[c^1_i,c^2_i,...,c^i_i,...,c^f_i] \\tag {6} ci​​ci​​​ci​=[ci1​,ci2​,...,cii​,...,cif​](6) 其中cifc^f_icif​是卷积滤波器 fff 产生的上下文特征。因此，利用等式(5)，我们可以准确地捕捉到用户评论文本和项目评论文本的上下文特征。 U=[c1u,c2u,c3u,...,cluu]\\pmb {U}=[\\pmb {c}_1^u,\\pmb {c}_2^u,\\pmb {c}_3^u,...,\\pmb {c}_{l_u}^u] UUU=[ccc1u​,ccc2u​,ccc3u​,...,ccclu​u​] (7)V=[c1i,c2i,c3i,...,clii]\\pmb {V}=[\\pmb {c}_1^i,\\pmb {c}_2^i,\\pmb {c}_3^i,...,\\pmb {c}_{l_i}^i] \\tag {7} VVV=[ccc1i​,ccc2i​,ccc3i​,...,cccli​i​](7) 其中，cku\\pmb c^u_kcccku​ 和 cji\\pmb c^i_jcccji​ 分别是用户评论文本和项目评论文本中第 kkk 个词和第 jjj 个词的上下文特征向量，lul_ulu​ 和 lil_ili​ 分别是用户评论文本和项目评论文本的长度。 Mutual attention layer. 与[23，26]类似，我们利用相互关注层来研究用户评论和项目评论之间的相关性。具体来说，我们定义了一个相关性评分函数 frelation−scoref_{relation-score}frelation−score​，它利用欧几里得距离来计算用户上下文特征 UUU 和与项目上下文特征 VVV 之间的相关性，相关性评分可以计算如下。 (8)frelation−score=1/(1+∣cku−cji∣)f_{relation-score}=1/(1+|\\pmb c^u_k-\\pmb c^i_j|) \\tag{8} frelation−score​=1/(1+∣cccku​−cccji​∣)(8) 根据相关评分函数，由式(8)可以得到用户-物品对相互关注矩阵 A∈RM×NA∈\\mathbb R^{M×N}A∈RM×N 。 (9)A=frelation−score(U,V)\\pmb A=f_{relation-score}(U,V) \\tag{9} AAA=frelation−score​(U,V)(9) AAA 中的每个元素表示用户-物品特征对的相关性，相关性矩阵中的每一行 Ak,∗A_{k,∗}Ak,∗​ 表示 UUU 中每个上下文特征 ckuc^u_ kcku​ 与 VVV 中的上下文特征的相关性，同理，A∗,jA_{*,j}A∗,j​表示 VVV 中每个上下文特征 cjic^i_jcji​ 与 UUU 中的上下文特征的相关性，上下文特征 ckuc^u_kcku​ 和 cjic^i_jcji​ 的相关性权重 gkug^u_kgku​ 和 gjig^i_jgji​ 可计算如下: gku=∑A[k,:]g^u_k=\\sum A[k,:] gku​=∑A[k,:] (10)gji=∑A[:,j]g^i_j=\\sum A[:,j] \\tag{10} gji​=∑A[:,j](10) Local pooling layer. 受工作[28]的启发，我们引入注意力加权作为替代方案，但利用平均池化作为基线，具体如下。具体来说，对于相互关注层的输出特征图，我们对滑动窗口连续行进行逐行平均，以生成更高粒度的抽象特征，我们可以得到具有相互关注权重的上下文特征。 tku=∑k=k:k+ωgku⋅cku\\pmb t^u_k=\\displaystyle \\sum _{k=k:k+\\omega}g^u_k \\cdot \\pmb c ^u_k tttku​=k=k:k+ω∑​gku​⋅cccku​ (11)tji=∑j=j:j+ωgji⋅cji\\pmb t^i_j=\\displaystyle \\sum _{j=j:j+\\omega}g^i_j \\cdot \\pmb c ^i_j \\tag{11} tttji​=j=j:j+ω∑​gji​⋅cccji​(11) 其中，tku∈Uu=[t1u,t2u,t3u,...,tluu]\\pmb t^u_k∈U^u = [\\pmb t^u_1 ,\\pmb t^u_2,\\pmb t^u_3, ... ,\\pmb t^u_{l_u}]tttku​∈Uu=[ttt1u​,ttt2u​,ttt3u​,...,tttlu​u​] 和 tji∈Vi=[t1i,t2i,t3i,...,tlii]\\pmb t^i_j∈V^i = [\\pmb t^i_1 ,\\pmb t^i_2,\\pmb t^i_3, ... ,\\pmb t^i_{l_i}]tttji​∈Vi=[ttt1i​,ttt2i​,ttt3i​,...,tttli​i​]分别表示用户评论和项目评论的第 kkk 个位置和第 jjj 个位置的上下文特征。为了提取更多的抽象特征，降低不相关方面的噪声，我们在局部池化层上叠加卷积层和平均池化层。抽象特征的计算方法如下。 hhj=δ(WajUh:h+ω−1u+baj)h^j_h=\\delta(\\pmb W^j_a\\pmb U^u_{h:h+\\omega-1}+b^j_a) hhj​=δ(WWWaj​UUUh:h+ω−1u​+baj​) hh=mean(h1j,...,hlu−ω+1j)h_h=mean(h^j_1,...,h^j_{{l_u-\\omega+1}}) hh​=mean(h1j​,...,hlu​−ω+1j​) (12)hu=[h1,...,hf]\\pmb h^u=[h_1,...,h_f] \\tag{12} hhhu=[h1​,...,hf​](12) 其中，Waj\\pmb W^j_aWWWaj​ 表示第 jjj 个卷积滤波器的卷积权重，滑动窗口的大小为ωωω，bajb^j_abaj​为偏置，第 jjj 个卷积滤波器在 hhh 位置的特征为 hhjh^j_hhhj​ 。最终得到用户与物品相关性的上下文特征 hu\\pmb h^uhhhu。同理，我们可以得到项目上下文特征 hih_ihi​ 。最后，我们使用非线性函数将上下文特征映射到维度空间，完成推荐任务。 hu=δ(Wuhu+bu)\\pmb h^u=\\delta(\\pmb W^u\\pmb h^u+b^u) hhhu=δ(WWWuhhhu+bu) (13)hi=δ(Wihi+bi)\\pmb h^i=\\delta(\\pmb W^i\\pmb h^i+b^i) \\tag{13} hhhi=δ(WWWihhhi+bi)(13) 其中，Wu\\pmb W^uWWWu和 Wi\\pmb W^iWWWi 分别表示用户和物品上下文特征的权重矩阵，bub^ubu 和 bib^ibi 表示偏差。 4.1.2 基于评分的特征学习 取one-hot编码的物品和用户ID作为物品特征向量 νiIν^I_iνiI​ 和用户特征向量 νuUν^U_uνuU​ 分别描述用户和物品。然后将特征向量 νiIν^I_iνiI​ 和 νuUν^U_uνuU​ 通过嵌入层中的潜因子矩阵 Pu∈RM×K\\pmb P_u∈\\mathbb R^{M×K}PPPu​∈RM×K 和 Qi∈RN×K\\pmb Q_i∈\\mathbb R^{N×K}Q​Q​​Qi​∈RN×K 映射成低维密集的潜因子向量，其表达式如下: pu=PuTνuU\\pmb p_u=\\pmb P^T_uν^U_u p​p​​pu​=PPPuT​νuU​ (14)qi=QiTνiI\\pmb q_i=\\pmb Q^T_iν^I_i \\tag {14} q​q​​qi​=Q​Q​​QiT​νiI​(14) 其中 pu\\pmb p_up​p​​pu​ 和 qi\\pmb q_iq​q​​qi​ 分别表示用户和物品的交互特征。 4.2 特征交互模块 为了全面捕捉用户偏好和物品特征，我们首先在特征交互模块中对两种异构信息特征进行融合。融合后的用户和物品特征可以表示为： uu=hu+pu\\pmb u^u=\\pmb h^u+\\pmb p_u uuuu=hhhu+p​p​​pu​ (15)νi=hi+qi\\pmb ν^i=\\pmb h^i+\\pmb q_i \\tag{15} νννi=hhhi+q​q​​qi​(15) 其中 uu\\pmb u^uuuuu 和 νi\\pmb ν^iνννi 分别表示最终的用户和物品特征。我们通过对特征的连接来进行组合。 (16)z=δ[u,ν]\\pmb z=\\delta [\\pmb u, \\pmb ν] \\tag{16} zzz=δ[uuu,ννν](16) 其中 z\\pmb zzzz 表示用户-物品特征， δδδ 为激活函数，[u,ν][\\pmb u, \\pmb ν][uuu,ννν] 在隐藏层中通过连通将两个特征结合起来。受工作[7]的启发，我们利用神经因子化机来捕捉特征的高阶非线性交互，目标函数表示为： (17)r^u,i(z)=m0+∑j=1∣z∣mjzj+f(z)\\hat r_{u,i}(z)=m_0+\\displaystyle \\sum^{|z|}_{j=1}m_jz_j+f(\\pmb z) \\tag{17} r^u,i​(z)=m0​+j=1∑∣z∣​mj​zj​+f(zzz)(17) 其中 z∈R∣z∣z∈\\mathbb R^{|z|}z∈R∣z∣ 表示用户项特征向量，zj∈zz_j∈\\pmb zzj​∈zzz 为特征j的值，m0m_0m0​ 表示全局偏差，mjm_jmj​ 表示潜伏特征向量的系数，f(z)f(\\pmb z)f(zzz) 建立特征的高阶交互模型，可以表示为： (18)f(z)=12[(∑j=1∣z∣zjνj)2−(∑k∣z∣zkνk)2]f(\\pmb z)={1\\over 2}[(\\displaystyle \\sum^{|\\pmb z|}_{j=1}z_j ν_j)^2-(\\displaystyle \\sum^{|\\pmb z|}_{k}z_k ν_k)^2] \\tag{18} f(zzz)=21​[(j=1∑∣zzz∣​zj​νj​)2−(k∑∣zzz∣​zk​νk​)2](18) 其中 zj，zk∈zz_j ，z_k∈\\pmb zzj​，zk​∈zzz 表示第 jjj 个和第 kkk 个用户项特征值，νj，νk∈Rsν_j ，ν_k∈\\mathbb R^sνj​，νk​∈Rs 表示特征 jjj 和 kkk 的嵌入向量，sss 为嵌入维度。与[27，29]类似，我们通过叠加多层全连接层来捕捉特征之间的高阶交互关系，最终的预测目标函数可以表示如下： (19)ru,i(z)=m0+∑j=1∣z∣mjzj+hTδL(WL(⋯δ1(W1f(z)+b1⋯ )+bL)r_{u,i}(\\pmb z)=m_0+ \\displaystyle \\sum _ {j=1}^ {|z|} m_ {j} z_ {j} + h^ {T}\\delta _ {L} ( W_ {L} ( \\cdots \\delta _ {1} ( W_ {1} f(z)+ b_ {1}\\cdots )+ b_ {L} ) \\tag{19} ru,i​(zzz)=m0​+j=1∑∣z∣​mj​zj​+hTδL​(WL​(⋯δ1​(W1​f(z)+b1​⋯)+bL​)(19) 其中 ru,i(z)r_{u,i}(\\pmb z)ru,i​(zzz) 表示预测得分，模型参数 Θ=m0，mj，vj，h，WL，bLΘ={m_0，{m_j ，v_j }，h，{W_L，b_L}}Θ=m0​，mj​，vj​，h，WL​，bL​ 。δLδ_LδL​ 代表激活函数。与FM相比，增加的参数 WL，bL{W_L，b_L}WL​，bL​ 主要用于学习特征的高阶交互。 4.3 联合训练 为了学习DAML模型的参数，我们利用回归与平方损失作为目标函数： (20)J=∑(u,i)∈R(r^u,i−ru,i)2+λΘ∣∣Θ∣∣2J=\\displaystyle \\sum _{(u,i)∈R}(\\hat r_{u,i}-r_{u,i})^2+\\lambda_\\Theta||\\Theta||^2 \\tag{20} J=(u,i)∈R∑​(r^u,i​−ru,i​)2+λΘ​∣∣Θ∣∣2(20) 其中 RRR 为用户-物品评分矩阵，ru,ir_{u,i}ru,i​ 为用户 uuu 对物品 iii 的真实评分，r^u,i\\hat r_{u,i}r^u,i​ 为预测评分，ΘΘΘ 表示所有参数，λΘ∣∣Θ∣∣2λ_Θ||Θ||^2λΘ​∣∣Θ∣∣2 作为正则化，防止模型过拟合。通过使用端到端的范式反向传播，可以有效地训练整个框架。算法1说明了DAML模型的训练过程。 DAML的参数基于公式（20）进行优化，采用随机梯度下降法（SGD）和反向传播法。也就是说，两个学习模块的参数是联合学习的。在参数更新方面，我们利用自适应矩估计(Adam)[10]进行小批量更新。此外，为了防止过拟合，我们对神经因子化机器采用了dropout策略[20]。 4.4 时间复杂度分析 在局部注意力层中，时间复杂度为 O(p(luM+liN))O(p(l_uM+l_iN))O(p(lu​M+li​N)) ，其中 MMM 和 NNN 分别表示用户和物品评论的数量，ppp 为内嵌维度，lul_ulu​ 和 lil_ili​ 分别表示用户和物品评论的长度。 在卷积层中，更新权重和偏置变量的时间复杂度为 O(p(mluM+nliN))O(p(ml_uM+nl_iN))O(p(mlu​M+nli​N)) ，其中 mmm 和 nnn 分别表示用户和项目上下文特征的数量。 在相互关注层中，计算用户与物品相关性的时间复杂度为 O(p(mluM+nliN))O(p(ml_uM +nl_iN))O(p(mlu​M+nli​N)) 。 对于特征交互模块，特征交互的时间复杂度为 O(d(m+n))O(d(m+n))O(d(m+n)) ，其中 ddd 为交互特征的维度。 对于隐藏层，矩阵向量乘法是主要的操作，可以在 O(dl−1dl)O(d_{l-1}d_l )O(dl−1​dl​) 中完成，其中 dld_ldl​ 表示第 lll 个隐藏层的维度。预测层的复杂度为 O(dL)O(d_L)O(dL​) 。 因此，总的时间复杂度为 O(p(2mluM+2nliN+luM+liN)+d(m+n)+∑l=1Ldl−1dl)O(p(2ml_uM+2nl_iN+l_uM+l_iN)+d(m+n)+\\sum^L_ {l=1} d_{l-1}d_l )O(p(2mlu​M+2nli​N+lu​M+li​N)+d(m+n)+∑l=1L​dl−1​dl​) 。可以看出，本文提出的DAML模型的时间复杂度主要与用户和项目数量固定时的潜伏特征的维度和数量有关。 ","link":"https://sourxi.github.io/post/daml-dual-attention-mutual-learning-between-ratings-and-reviews-for-item-recommendation/"}]}